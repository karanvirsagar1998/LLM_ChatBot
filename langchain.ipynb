{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "hf_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "# Note: pipeline is not directly compatible with LLMChain â†’ You need to wrap it using HuggingFacePipeline.\n",
    "# LangChain expects structured output, while pipeline returns a list of dictionaries (which must be handled properly).\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline) #use this to create a LLM chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "#define prompt here\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template = \"Tell me the capital of  {country} ?\")\n",
    "\n",
    "formatted_prompt = prompt_template.format(country='India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chennai\n"
     ]
    }
   ],
   "source": [
    "# Firstly, directly using the pipeline here without any chains\n",
    "response = hf_pipeline(formatted_prompt)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3788/1650880420.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm = llm, prompt = prompt_template)\n",
      "/tmp/ipykernel_3788/1650880420.py:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(\"India\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chennai'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chains concept here\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template)\n",
    "chain.run(\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining multiple chains here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template=PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template=\"Please tell me the capital of the {country}\"\n",
    ")\n",
    "capital_chain = LLMChain(llm=llm, prompt=capital_template)\n",
    "#I create two chains, above abnd below\n",
    "famous_template = PromptTemplate(\n",
    "    input_variables=['capital'],\n",
    "    template=\"Suggest me some amazing places to visit in {capital}\"\n",
    ")\n",
    "famous_chain = LLMChain(llm = llm, prompt=famous_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Toronto Museum of Art'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get outpt from the first chain and pass it to the another chain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains=[capital_chain, famous_chain])\n",
    "chain.run(\"Canada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting answers of all the chains\n",
    "capital_template=PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template=\"Please tell me the capital of the {country}\"\n",
    ")\n",
    "capital_chain = LLMChain(llm=llm, prompt=capital_template, output_key=\"capital\")\n",
    "\n",
    "famous_template = PromptTemplate(\n",
    "    input_variables=['capital'],\n",
    "    template=\"Suggest me some amazing places to visit in {capital}\"\n",
    ")\n",
    "famous_chain = LLMChain(llm = llm, prompt=famous_template, output_key=\"places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3788/1546279761.py:8: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain({'country: \"India'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'country': {'country: \"India'},\n",
       " 'capital': 'calcutta',\n",
       " 'places': 'The National Museum of India'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain=SequentialChain(\n",
    "    chains=[capital_chain, famous_chain],\n",
    "    input_variables=[\"country\"],\n",
    "    output_variables=[\"capital\",\"places\"]\n",
    "    )\n",
    "chain({'country: \"India'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3788/2086509714.py:11: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  Chatllm(final_input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI assistants are a joke'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline #for hugging face models\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "Chatllm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a comedian AI assistant\")\n",
    "human_message = HumanMessage(content=\"Please make some jokes on AI\")\n",
    "\n",
    "# Join the content of the messages into one string\n",
    "final_input = \"\\n\".join([system_message.content, human_message.content])\n",
    "\n",
    "Chatllm(final_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template + LLM + Outout Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commaseperatedoutput(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a chat prompt\n",
    "template=\"You area  helpful assistant. When the use of given any input, you should generate five words sysnsynonym  in a comma seperated list\"\n",
    "human_template = \"{text}\"\n",
    "chatprompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template), \n",
    "    (\"human\", human_template)\n",
    "])\n",
    "\n",
    "chain = chatprompt|Chatllm|Commaseperatedoutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good human']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"good\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
